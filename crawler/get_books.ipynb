{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://search.books.com.tw/search/query/cat/1/key/{keyword}/sort/1/ms2/ms2_1/page/{page}/v/0/\"\n",
    "store = [\n",
    "  'books',\n",
    "  'eslite',\n",
    "  'kingstone',\n",
    "  'sanmin',\n",
    "]\n",
    "READ = 'r'\n",
    "WRITE = 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('eslite.txt', 'r',encoding='utf-8') as fp:\n",
    "    data = eval(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('eslite.txt', 'w',encoding='utf-8') as fp:\n",
    "    fp.write(str(tmp_data))\n",
    "    isSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "with open('kingstone.txt', 'r',encoding='utf-8') as fp:\n",
    "    data = eval(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_data.update(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2865"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data ={}\n",
    "for i in data:\n",
    "    tmp = data[i]\n",
    "    if i!=None:\n",
    "        tmp_data[i.strip()]={\n",
    "            'imgURL': tmp['imgURL'].strip(),\n",
    "            'title': tmp['title'].strip(),\n",
    "            'price': tmp['price'].strip(),\n",
    "            'date': tmp['date'].strip(),  # 有沒此日期(其他商品)\n",
    "            'author': tmp['author'].strip(),\n",
    "            'pub': tmp['pub'].strip(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "記得換頁\n"
     ]
    }
   ],
   "source": [
    "# 主程式\n",
    "try:\n",
    "    isSave = False\n",
    "    for i in keyword:\n",
    "        for j in range(6, 11): # 2~last page #全部完成後，你要改(1~15), 一次5頁\n",
    "            try:\n",
    "                tmp_url = url.format(keyword=i, page=j)\n",
    "                soup = getHtml(method='get', url=tmp_url)\n",
    "                if soup !=None:\n",
    "                    index = None\n",
    "                    item_soup = soup.find('ul', class_='searchbook').findAll('li', class_=\"item\")\n",
    "                    for k in item_soup:\n",
    "                        index = k.find('input')\n",
    "                        if index!=None:\n",
    "                            index = index['value']\n",
    "                        else:\n",
    "                            try:\n",
    "                                href = k.find('a', {'rel':'mid_image'})['href']\n",
    "                                tmp_index = re.split('item/|/page',href)[1]\n",
    "                                index = tmp_index\n",
    "                            except:\n",
    "                                print('取得item index錯誤...')\n",
    "                        try:\n",
    "                            if index not in data:\n",
    "                                data[index] = {\n",
    "                                    'imgURL': k.find('img')['data-original'],\n",
    "                                    'title': k.find('h3').a['title'],\n",
    "                                    'price': k.find('span', class_='price').findAll('strong')[-1].b.text,\n",
    "                                    'date': k.find(text=re.compile('出版日期')).split(':')[-1].strip().replace('-','/'),  # 有沒此日期(其他商品)\n",
    "                                    'author': ', '.join([z['title'] for z in k.findAll('a', {'rel': 'go_author'})]),\n",
    "                                    'pub': k.find('a', {'rel': 'mid_publish'}).text,\n",
    "                                }\n",
    "                        except:\n",
    "                            pass\n",
    "                else:\n",
    "                    print('soup none')\n",
    "                    history.append({'connect':None,'key':i, 'page':j})\n",
    "\n",
    "            except:\n",
    "                print('Error')\n",
    "                history.append({'key':i, 'page':j})\n",
    "            time.sleep(0.3)\n",
    "        time.sleep(120)\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "\n",
    "print('記得換頁')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "other item\n",
      "other item\n",
      "other item\n",
      "other item\n"
     ]
    }
   ],
   "source": [
    "tmp_url = url.format(keyword='django', page=7)\n",
    "soup = getHtml(method='get', url=tmp_url)\n",
    "if soup !=None:\n",
    "    index = None\n",
    "    item_soup = soup.find('ul', class_='searchbook').findAll('li', class_=\"item\")\n",
    "    for k in item_soup:\n",
    "        index = k.find('input')\n",
    "        if index!=None:\n",
    "            index = index['value']\n",
    "        else:\n",
    "            try:\n",
    "                href = k.find('a', {'rel':'mid_image'})['href']\n",
    "                tmp_index = re.split('item/|/page',href)[1]\n",
    "                index = tmp_index\n",
    "            except:\n",
    "                print('取得item index錯誤...')\n",
    "        try:\n",
    "            if index not in data:\n",
    "                data[index] = {\n",
    "                    'imgURL': k.find('img')['data-original'],\n",
    "                    'title': k.find('h3').a['title'],\n",
    "                    'price': k.find('span', class_='price').findAll('strong')[-1].b.text,\n",
    "                    'date': k.find(text=re.compile('出版日期')).split(':')[-1].strip().replace('-','/'),\n",
    "                    'author': ', '.join([z['title'] for z in k.findAll('a', {'rel': 'go_author'})]),\n",
    "                    'pub': k.find('a', {'rel': 'mid_publish'}).text,\n",
    "                }\n",
    "        except:\n",
    "            print('other item')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# from fake_useragent import UserAgent\n",
    "import re, requests, json, time, urllib\n",
    "def getHtml(method, url, params= None, data= None, encoding= 'utf-8'):\n",
    "    try:\n",
    "#         headers = {'user-agent':str(UserAgent().random)}\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36 Edg/86.0.622.69\"}\n",
    "        if method == 'get':\n",
    "            req = requests.get(url, headers= headers, params= params, timeout=10000)\n",
    "        elif method == 'post':\n",
    "            req = requests.post(url, headers= headers, data= data, timeout=10000)\n",
    "        if req.status_code == 200:\n",
    "            print('requests -> 200')\n",
    "            req.encoding = encoding\n",
    "            return BeautifulSoup(req.text, 'html.parser')\n",
    "        print('requests -> ', req.status_code)\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "requests -> 200\n"
     ]
    }
   ],
   "source": [
    "# 主程式\n",
    "current = {\n",
    "    'index':None,\n",
    "    'page':None\n",
    "}\n",
    "for page in range(16, 18): # 2~last page #全部完成後，你要改(1~15), 一次5頁\n",
    "    current['page']=page\n",
    "    tmp_url = url.format(keyword='python', page=page)\n",
    "    soup = getHtml(method='get', url=tmp_url)\n",
    "    if soup !=None:\n",
    "        index = None\n",
    "        item_soup = soup.find('ul', class_='searchbook').findAll('li', class_=\"item\")\n",
    "        for k in item_soup:\n",
    "            index = k.find('input')\n",
    "            if index!=None:\n",
    "                index = index['value']\n",
    "            else:\n",
    "                try:\n",
    "                    href = k.find('a', {'rel':'mid_image'})['href']\n",
    "                    tmp_index = re.split('item/|/page',href)[1]\n",
    "                    index = tmp_index\n",
    "                except:\n",
    "                    print('取得item index錯誤...')\n",
    "            current['index'] = index\n",
    "            try:\n",
    "                if index not in data:\n",
    "                    data[index] = {\n",
    "                        'imgURL': k.find('img')['data-original'],\n",
    "                        'title': k.find('h3').a['title'],\n",
    "                        'price': k.find('span', class_='price').findAll('strong')[-1].b.text,\n",
    "                        'date': k.find(text=re.compile('出版日期')).split(':')[-1].strip().replace('-','/'),  # 有沒此日期(其他商品)\n",
    "                        'author': ', '.join([z['title'] for z in k.findAll('a', {'rel': 'go_author'})]),\n",
    "                        'pub': k.find('a', {'rel': 'mid_publish'}).text,\n",
    "                    }\n",
    "            except:\n",
    "                print('pass')\n",
    "                pass\n",
    "    else:\n",
    "        print('soup none')\n",
    "        history.append({'connect':None,'key':i, 'page':j})\n",
    "\n",
    "\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.books.com.tw/products/0010869590?sloc=main'\n",
    "soup = getHtml(method='get',url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590.jpg&v=5f4e22b0&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_bc_01.jpg&v=5f4e22ec&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_01.jpg&v=5f4e2314&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_02.jpg&v=5f4e2314&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_03.jpg&v=5f4e22ef&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_04.jpg&v=5f4e2310&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_05.jpg&v=5f4e231a&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_06.jpg&v=5f4e22e9&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_07.jpg&v=5f4e22e9&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_08.jpg&v=5f4e2313&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_09.jpg&v=5f4e230b&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_10.jpg&v=5f4e22ec&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_11.jpg&v=5f4e2307&w=348&h=348',\n",
       " 'https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_b_12.jpg&v=5f4e22ea&w=348&h=348',\n",
       " 'https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/086/95/0010869590_bf_01.jpg&v=5f4e22fa&w=348&h=348']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('ul', {'style':'float:left;'}):\n",
    "    for j in i.find_all('img'):\n",
    "        b.append(j['src'].split('&amp')[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
