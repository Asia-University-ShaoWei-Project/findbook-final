{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sanmin.com.tw/search/index?ct=K&K={keyword}&ls=SD&vs=list&pi={page}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "with open('sanmin.txt', 'r',encoding='utf-8') as fp:\n",
    "    data1 = eval(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存資料\n",
    "with open('sanmin2.txt', 'w',encoding='utf-8') as fp:\n",
    "    fp.write(str(data))\n",
    "    isSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = [f\"%E5%8B%95%E7%89%A9\", f\"%E9%9B%BB%E8%85%A6\", f\"%E7%A7%91%E6%8A%80\",f\"%E7%A7%91%E5%AD%B8\",f\"%E9%9B%BB%E5%AD%90\",]\n",
    "# 動物, 電腦, 科技, 科學, 電子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "date error\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "date error\n",
      "requests -> 200\n",
      "date error\n",
      "requests -> 200\n"
     ]
    }
   ],
   "source": [
    "current = {\n",
    "    'index':None,\n",
    "    'key':None,\n",
    "    'page':None\n",
    "}\n",
    "isSave = False\n",
    "for key in keyword:\n",
    "    \n",
    "    for page in range(6, 11): # 2~last page\n",
    "        tmp_url = url.format(keyword=key, page=page)\n",
    "        current['key']=key\n",
    "        current['page']=page\n",
    "        soup = getHtml(method='get', url=tmp_url)\n",
    "        if soup!=None:\n",
    "            item_soup = soup.find('div', id='normal-list').find_all('div', class_='condition')\n",
    "            for i in item_soup:\n",
    "                title = i.h3.a\n",
    "                index = title['href'].split('/')[-1].strip()\n",
    "                current['index']=index\n",
    "                \n",
    "                for word_index, j in enumerate(title.text):\n",
    "                    if j=='.':\n",
    "                        title = title.text[word_index+1:]\n",
    "                        break\n",
    "\n",
    "                if index not in data:\n",
    "                    price = i.find('span', class_='price')\n",
    "                    if price !=None:\n",
    "                        price = price.text\n",
    "                    else:\n",
    "                        try:\n",
    "                            new_crazy = filter(str.isdigit, i.find('div', class_='resultBooksLayout').p.text.strip())\n",
    "                            price = ''.join(list(new_crazy))\n",
    "                        except: # 完全沒價錢\n",
    "                            price = ''\n",
    "                    author = i.find('span', class_='ProdAu')\n",
    "                    if author!=None:\n",
    "                        author= ', '.join([j.text.strip() for j in author.find_all('a')]).strip(),\n",
    "                    else:    \n",
    "                        author=''\n",
    "                        \n",
    "                    date = i.find('p', class_='author')\n",
    "                    if date!=None:\n",
    "                        try:\n",
    "                            date= date.find_all('span', class_='pap')[-1].contents[-1].strip()\n",
    "                        except:\n",
    "                            print('date error')\n",
    "                            history.append({'status':'process', 'index':index, 'key':key, 'page':page})\n",
    "                            date = ''\n",
    "                    else:    \n",
    "                        date=''\n",
    "                    \n",
    "                    pub = i.find('span', class_='ProdPu')\n",
    "                    if pub!=None:\n",
    "                        pub= pub.a.text.strip()\n",
    "                    else:    \n",
    "                        pub=''\n",
    "\n",
    "                    data[index] = {\n",
    "                        'imgURL':i.find('img', class_=\"lazyload\")['original'].strip(),\n",
    "                        'title':title.strip(),\n",
    "                        'price':price.strip() if price.strip()!=None else \"0\",\n",
    "                        'date':date,\n",
    "                        'author':author,\n",
    "                        'pub':pub\n",
    "                    }\n",
    "        else:\n",
    "            print('soup none')\n",
    "            history.append({'status':'connect', 'index':history_index, 'key':key, 'page':page})\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "    time.sleep(120)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '000224270',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 5},\n",
       " {'status': 'process',\n",
       "  'index': '000084351',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 5},\n",
       " {'status': 'connect',\n",
       "  'index': '007691416',\n",
       "  'key': '%E9%A7%AD%E5%AE%A2',\n",
       "  'page': 6},\n",
       " {'status': 'process',\n",
       "  'index': '000200113',\n",
       "  'key': '%E6%BC%AB%E7%95%AB',\n",
       "  'page': 5},\n",
       " {'status': 'process',\n",
       "  'index': '007682343',\n",
       "  'key': '%E7%96%AB%E6%83%85',\n",
       "  'page': 8},\n",
       " {'status': 'process',\n",
       "  'index': '002549703',\n",
       "  'key': '%E7%96%AB%E6%83%85',\n",
       "  'page': 10},\n",
       " {'status': 'process',\n",
       "  'index': '002549704',\n",
       "  'key': '%E7%96%AB%E6%83%85',\n",
       "  'page': 10}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '003865894',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 1},\n",
       " {'status': 'process',\n",
       "  'index': '000224270',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 5},\n",
       " {'status': 'process',\n",
       "  'index': '000084351',\n",
       "  'key': '%E5%81%A5%E5%BA%B7',\n",
       "  'page': 5},\n",
       " {'status': 'connect',\n",
       "  'index': '007691416',\n",
       "  'key': '%E9%A7%AD%E5%AE%A2',\n",
       "  'page': 6}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://www.sanmin.com.tw/search/index?ct=K&K=%E7%96%AB%E6%83%85&ls=SD&vs=List&pi=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.sanmin.com.tw/search/index?ct=K&K={keyword}&ls=SD&vs=list&pi={page}\".format(keyword='python', page=136)\n",
    "soup = getHtml(method='get', url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '001018941', 'key': '%E6%BC%AB%E7%95%AB', 'page': 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "tmp_url = url.format(keyword=\"%E7%96%AB%E6%83%85\", page=8)\n",
    "soup = getHtml(method='get', url=tmp_url)\n",
    "if soup!=None:\n",
    "    item_soup = soup.find('div', id='normal-list').find_all('div', class_='condition')\n",
    "    for i in item_soup:\n",
    "        title = i.h3.a\n",
    "        index = title['href'].split('/')[-1].strip()\n",
    "        current['index']=index\n",
    "\n",
    "        for word_index, j in enumerate(title.text):\n",
    "            if j=='.':\n",
    "                title = title.text[word_index+1:]\n",
    "                break\n",
    "\n",
    "        if index not in data:\n",
    "            price = i.find('span', class_='price')\n",
    "            if price !=None:\n",
    "                price = price.text\n",
    "            else:\n",
    "                try:\n",
    "                    new_crazy = filter(str.isdigit, i.find('div', class_='resultBooksLayout').p.text.strip())\n",
    "                    price = ''.join(list(new_crazy))\n",
    "                except: # 完全沒價錢\n",
    "                    price = ''\n",
    "            author = i.find('span', class_='ProdAu')\n",
    "            if author!=None:\n",
    "                author= ', '.join([j.text.strip() for j in author.find_all('a')]).strip(),\n",
    "            else:    \n",
    "                author=''\n",
    "\n",
    "            date = i.find('p', class_='author')\n",
    "            if date!=None:\n",
    "                try:\n",
    "                    date= date.find_all('span', class_='pap')[-1].contents[-1].strip()\n",
    "                except:\n",
    "                    print('date error')\n",
    "                    history.append({'status':'process', 'index':index, 'key':key, 'page':page})\n",
    "                    date = ''\n",
    "            else:    \n",
    "                date=''\n",
    "\n",
    "            pub = i.find('span', class_='ProdPu')\n",
    "            if pub!=None:\n",
    "                pub= pub.a.text.strip()\n",
    "            else:    \n",
    "                pub=''\n",
    "            print(index)\n",
    "            print(i.find('img', class_=\"lazyload\")['original'].strip())\n",
    "            print(title.strip())\n",
    "            print(price.strip() if price.strip()!=None else \"0\")\n",
    "            print(date)\n",
    "            print(author)\n",
    "            print(pub)\n",
    "            print('------------------')\n",
    "        else:\n",
    "            print('in')\n",
    "else:\n",
    "    print('soup none')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, requests, json, time\n",
    "\n",
    "def getHtml(method, url, params= None, data= None, encoding= 'utf-8'):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36 Edg/86.0.622.69\"}\n",
    "        if method == 'get':\n",
    "            req = requests.get(url, headers= headers, params= params)\n",
    "        elif method == 'post':\n",
    "            req = requests.post(url, headers= headers, data= data)\n",
    "        if req.status_code == 200:\n",
    "            print('requests -> 200')\n",
    "            req.encoding = encoding\n",
    "            return BeautifulSoup(req.text, 'html.parser')\n",
    "        print('requests -> ', req.status_code)\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n",
      "requests -> 200\n"
     ]
    }
   ],
   "source": [
    "current = {\n",
    "    'index':None,\n",
    "    'page':None\n",
    "}\n",
    "isSave = False\n",
    "\n",
    "for page in range(15, 20): # 2~last page\n",
    "    tmp_url = url.format(keyword='python', page=page)\n",
    "    current['page']=page\n",
    "    soup = getHtml(method='get', url=tmp_url)\n",
    "    if soup!=None:\n",
    "        item_soup = soup.find('div', id='normal-list').find_all('div', class_='condition')\n",
    "        for i in item_soup:\n",
    "            title = i.h3.a\n",
    "            index = title['href'].split('/')[-1].strip()\n",
    "            current['index']=index\n",
    "\n",
    "            for word_index, j in enumerate(title.text):\n",
    "                if j=='.':\n",
    "                    title = title.text[word_index+1:]\n",
    "                    break\n",
    "\n",
    "            if index not in data:\n",
    "                price = i.find('span', class_='price')\n",
    "                if price !=None:\n",
    "                    price = price.text\n",
    "                else:\n",
    "                    try:\n",
    "                        new_crazy = filter(str.isdigit, i.find('div', class_='resultBooksLayout').p.text.strip())\n",
    "                        price = ''.join(list(new_crazy))\n",
    "                    except: # 完全沒價錢\n",
    "                        price = ''\n",
    "                author = i.find('span', class_='ProdAu')\n",
    "                if author!=None:\n",
    "                    author= ', '.join([j.text.strip() for j in author.find_all('a')]).strip(),\n",
    "                else:    \n",
    "                    author=''\n",
    "\n",
    "                date = i.find('p', class_='author')\n",
    "                if date!=None:\n",
    "                    try:\n",
    "                        date= date.find_all('span', class_='pap')[-1].contents[-1].strip()\n",
    "                    except:\n",
    "                        print('date error')\n",
    "                        history.append({'status':'process', 'index':index, 'key':key, 'page':page})\n",
    "                        date = ''\n",
    "                else:    \n",
    "                    date=''\n",
    "\n",
    "                pub = i.find('span', class_='ProdPu')\n",
    "                if pub!=None:\n",
    "                    pub= pub.a.text.strip()\n",
    "                else:    \n",
    "                    pub=''\n",
    "\n",
    "                data[index] = {\n",
    "                    'imgURL':i.find('img', class_=\"lazyload\")['original'].strip(),\n",
    "                    'title':title.strip(),\n",
    "                    'price':price.strip() if price.strip()!=None else \"0\",\n",
    "                    'date':date,\n",
    "                    'author':author,\n",
    "                    'pub':pub\n",
    "                }\n",
    "    else:\n",
    "        print('soup none')\n",
    "        history.append({'status':'connect', 'index':history_index, 'key':key, 'page':page})\n",
    "\n",
    "    time.sleep(0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
